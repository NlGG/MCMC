{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph MambaModel {\n",
      "\t\"y\" [shape=\"ellipse\", fillcolor=\"gray85\", style=\"filled\"];\n",
      "\t\"mu\" [shape=\"diamond\", fillcolor=\"gray85\", style=\"filled\"];\n",
      "\t\t\"mu\" -> \"y\";\n",
      "\t\"s2\" [shape=\"ellipse\"];\n",
      "\t\t\"s2\" -> \"y\";\n",
      "\t\"beta\" [shape=\"ellipse\"];\n",
      "\t\t\"beta\" -> \"mu\";\n",
      "\t\"xmat\" [shape=\"box\", fillcolor=\"gray85\", style=\"filled\"];\n",
      "\t\t\"xmat\" -> \"mu\";\n",
      "}\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: MethodError: `mcmc` has no method matching mcmc(::Mamba.Model, ::Dict{Symbol,Any}, ::Array{Any,1}, ::Int64)\nClosest candidates are:\n  mcmc(::Mamba.Model, ::Dict{Symbol,V}, !Matched::Array{Dict{Symbol,Any},1}, ::Integer)\nwhile loading In[2], in expression starting on line 94",
     "output_type": "error",
     "traceback": [
      "LoadError: MethodError: `mcmc` has no method matching mcmc(::Mamba.Model, ::Dict{Symbol,Any}, ::Array{Any,1}, ::Int64)\nClosest candidates are:\n  mcmc(::Mamba.Model, ::Dict{Symbol,V}, !Matched::Array{Dict{Symbol,Any},1}, ::Integer)\nwhile loading In[2], in expression starting on line 94",
      ""
     ]
    }
   ],
   "source": [
    "using Mamba\n",
    "\n",
    "## Model and User-Defined Sampler Specifications\n",
    "\n",
    "model = Model(\n",
    "\n",
    "  y = Stochastic(1,\n",
    "    (mu, s2) ->  MvNormal(mu, sqrt(s2)),\n",
    "    false\n",
    "  ),\n",
    "\n",
    "  mu = Logical(1,\n",
    "    (xmat, beta) -> xmat * beta,\n",
    "    false\n",
    "  ),\n",
    "\n",
    "  beta = Stochastic(1,\n",
    "    () -> MvNormal(2, sqrt(1000))\n",
    "  ),\n",
    "\n",
    "  s2 = Stochastic(\n",
    "    () -> InverseGamma(0.001, 0.001)\n",
    "  )\n",
    "\n",
    ")\n",
    "\n",
    "Gibbs_beta = Sampler([:beta],\n",
    "  (beta, s2, xmat, y) ->\n",
    "    begin\n",
    "      beta_mean = mean(beta.distr)\n",
    "      beta_invcov = invcov(beta.distr)\n",
    "      Sigma = inv(xmat' * xmat / s2 + beta_invcov)\n",
    "      mu = Sigma * (xmat' * y / s2 + beta_invcov * beta_mean)\n",
    "      rand(MvNormal(mu, Sigma))\n",
    "    end\n",
    ")\n",
    "\n",
    "Gibbs_s2 = Sampler([:s2],\n",
    "  (mu, s2, y) ->\n",
    "    begin\n",
    "      a = length(y) / 2.0 + shape(s2.distr)\n",
    "      b = sumabs2(y - mu) / 2.0 + scale(s2.distr)\n",
    "      rand(InverseGamma(a, b))\n",
    "    end\n",
    ")\n",
    "\n",
    "\n",
    "## Hybrid No-U-Turn and Slice Sampling Scheme\n",
    "scheme1 = [NUTS(:beta),\n",
    "           Slice(:s2, 3.0)]\n",
    "\n",
    "## No-U-Turn Sampling Scheme\n",
    "scheme2 = [NUTS([:beta, :s2])]\n",
    "\n",
    "## User-Defined Sampling Scheme\n",
    "scheme3 = [Gibbs_beta, Gibbs_s2]\n",
    "\n",
    "\n",
    "## Sampling Scheme Assignment\n",
    "setsamplers!(model, scheme1)\n",
    "\n",
    "\n",
    "## Graph Representation of Nodes\n",
    "draw(model)\n",
    "draw(model, filename=\"lineDAG.dot\")\n",
    "\n",
    "\n",
    "## Data\n",
    "line = Dict{Symbol, Any}(\n",
    "  :x => [1, 2, 3, 4, 5],\n",
    "  :y => [1, 3, 3, 3, 5]\n",
    ")\n",
    "line[:xmat] = [ones(5) line[:x]]\n",
    "\n",
    "\n",
    "## Set Random Number Generator Seed\n",
    "srand(123)\n",
    "\n",
    "\n",
    "## Initial Values\n",
    "inits = [\n",
    "  Dict{Symbol, Any}(\n",
    "    :y => line[:y],\n",
    "    :beta => rand(Normal(0, 1), 2),\n",
    "    :s2 => rand(Gamma(1, 1))\n",
    "  )\n",
    "  for i in 1:3\n",
    "]\n",
    "\n",
    "\n",
    "## MCMC Simulations\n",
    "\n",
    "setsamplers!(model, scheme1)\n",
    "sim1 = mcmc(model, line, inits, 10000, burnin=250, thin=2, chains=3)\n",
    "\n",
    "setsamplers!(model, scheme2)\n",
    "sim2 = mcmc(model, line, inits, 10000, burnin=250, thin=2, chains=3)\n",
    "\n",
    "setsamplers!(model, scheme3)\n",
    "sim3 = mcmc(model, line, inits, 10000, burnin=250, thin=2, chains=3)\n",
    "\n",
    "\n",
    "## Gelman, Rubin, and Brooks Convergence Diagnostic\n",
    "gelmandiag(sim1, mpsrf=true, transform=true) |> showall\n",
    "\n",
    "## Geweke Convergence Diagnostic\n",
    "gewekediag(sim1) |> showall\n",
    "\n",
    "## Heidelberger-Welch Diagnostic\n",
    "heideldiag(sim1) |> showall\n",
    "\n",
    "## Raftery-Lewis Convergence Diagnostic\n",
    "rafterydiag(sim1) |> showall\n",
    "\n",
    "\n",
    "## Summary Statistics\n",
    "describe(sim1)\n",
    "\n",
    "## Highest Posterior Density Intervals\n",
    "hpd(sim1) |> show\n",
    "\n",
    "## Cross-Correlations\n",
    "cor(sim1) |> show\n",
    "\n",
    "## Lag-Autocorrelations\n",
    "autocor(sim1) |> show\n",
    "\n",
    "## State Space Change Rate (per Iteration)\n",
    "changerate(sim1) |> show\n",
    "\n",
    "## Deviance Information Criterion\n",
    "dic(sim1) |> show\n",
    "\n",
    "\n",
    "## Subset Sampler Output\n",
    "sim = sim1[1000:5000, [\"beta[1]\", \"beta[2]\"], :]\n",
    "describe(sim)\n",
    "\n",
    "\n",
    "## Write to and Read from an External File\n",
    "write(\"sim1.jls\", sim1)\n",
    "sim1 = read(\"sim1.jls\", ModelChains)\n",
    "\n",
    "\n",
    "## Restart the Sampler\n",
    "sim = mcmc(sim1, 5000)\n",
    "describe(sim)\n",
    "\n",
    "\n",
    "## Plotting\n",
    "\n",
    "## Default summary plot (trace and density plots)\n",
    "p = plot(sim1)\n",
    "\n",
    "## Write plot to file\n",
    "draw(p, filename=\"summaryplot.svg\")\n",
    "#draw(p, filename=\"summaryplot.pdf\", fmt=:pdf)\n",
    "\n",
    "## Autocorrelation and running mean plots\n",
    "p = plot(sim1, [:autocor, :mean], legend=true)\n",
    "draw(p, nrow=3, ncol=2, filename=\"autocormeanplot.svg\")\n",
    "#draw(p, nrow=3, ncol=2, filename=\"autocormeanplot.pdf\", fmt=:pdf)\n",
    "\n",
    "## Pairwise contour plots\n",
    "p = plot(sim1, :contour)\n",
    "draw(p, nrow=2, ncol=2, filename=\"contourplot.svg\")\n",
    "#draw(p, nrow=2, ncol=2, filename=\"contourplot.pdf\", fmt=:pdf)\n",
    "\n",
    "\n",
    "## Development and Testing\n",
    "\n",
    "setinputs!(model, line)             # Set input node values\n",
    "setinits!(model, inits[1])          # Set initial values\n",
    "setsamplers!(model, scheme1)        # Set sampling scheme\n",
    "\n",
    "showall(model)                      # Show detailed node information\n",
    "\n",
    "logpdf(model, 1)                    # Log-density sum for block 1\n",
    "logpdf(model, 2)                    # Block 2\n",
    "logpdf(model)                       # All blocks\n",
    "\n",
    "sample!(model, 1)                  # Sample values for block 1\n",
    "sample!(model, 2)                  # Block 2\n",
    "sample!(model)                     # All blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression Model\n",
    "--------------------------------\n",
    "\n",
    "このセクションでは、BUGS 0.5のマニュアルから引用した、ベイズによる単純線形回帰を用いてこのパッケージの特徴を説明する。この例は観測値${x} = (1, 2, 3, 4, 5)^\\top$と${y} = (1, 3, 3, 3, 5)^\\top$との間の回帰関係を記述する。これは次のように表現できる。\n",
    "\n",
    "$ \\bf{y}  \\sim Normal (\\bf{\\mu}, \\sigma^2 \\bf{I})$\n",
    "\n",
    "$\\bf{\\mu} = \\bf{X} \\bf{\\beta} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで事前分布は、\n",
    "\n",
    "$ \n",
    "\\bf{\\beta} \\sim \\it{Normal}(\n",
    "\\bf{\\mu}_\\pi =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix},\n",
    "\\bf{\\sigma}_\\pi =\n",
    "\\begin{bmatrix}\n",
    "1000 & 0 \\\\\n",
    "0 & 1000 \n",
    "\\end{bmatrix} )\n",
    "$\n",
    "\n",
    "\n",
    "$\\sigma^2 \\sim InverseGamma(\\alpha_\\pi = 0.001, \\beta_\\pi = 0.001)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $ \\bm{\\beta} = (\\beta_0, \\beta_1)^\\T $, $\\bm{X}$ は第1列に切片ベクトルを持ち、第2列以降に$x$ベクトルをもつ。$\\beta_1$ と$\\sigma^2$は、事後分布に基づいたパラメータである。この例における計算的な問題は、推論は直接的に結合事後分布から得られないということである。なぜなら　 because of its nonstandard form, derived below up to a constant of proportionality.\n",
    "\n",
    ".. math::\n",
    "\n",
    "    p(\\bm{\\beta}, \\sigma^2 | \\bm{y}) &\\propto p(\\bm{y} | \\bm{\\beta}, \\sigma^2) p(\\bm{\\beta}) p(\\sigma^2) \\\\\n",
    "    &\\propto \\left(\\sigma^2\\right)^{-n/2} \\exp\\left\\{-\\frac{1}{2 \\sigma^2} (\\bm{y} - \\bm{X} \\bm{\\beta})^\\top (\\bm{y} - \\bm{X} \\bm{\\beta}) \\right\\} \\\\\n",
    "    &\\quad \\times \\exp\\left\\{-\\frac{1}{2} (\\bm{\\beta} - \\bm{\\mu}_\\pi)^\\top \\bm{\\Sigma}_\\pi^{-1} (\\bm{\\beta} - \\bm{\\mu}_\\pi) \\right\\}\n",
    "    \\left(\\sigma^2\\right)^{-\\alpha_\\pi - 1} \\exp\\left\\{-\\beta_\\pi / \\sigma^2\\right\\}\n",
    "\n",
    "A common alternative is to make approximate inference based on parameter values simulated from the posterior with MCMC methods.\n",
    "\n",
    "\n",
    ".. _section-Line-Specification:\n",
    "\n",
    "Model Specification\n",
    "-------------------\n",
    "\n",
    "Nodes\n",
    "^^^^^\n",
    "\n",
    "In the `Mamba` package, terms that appear in the Bayesian model specification are referred to as *nodes*.  Nodes are classified as one of three types:\n",
    "\n",
    "    * **Stochastic nodes** are any model terms that have likelihood or prior distributional specifications.  In the regression example, :math:`\\bm{y}`, :math:`\\bm{\\beta}`, and :math:`\\sigma^2` are stochastic nodes.\n",
    "    * **Logical nodes** are terms, like :math:`\\bm{\\mu}`, that are deterministic functions of other nodes.\n",
    "    * **Input nodes** are any remaining model terms (:math:`\\bm{X}`) and are considered to be fixed quantities in the analysis.\n",
    "\n",
    "Note that the :math:`\\bm{y}` node has both a distributional specification and is a fixed quantity.  It is designated a stochastic node in `Mamba` because of its distributional specification.  This allows for the possibility of model terms with distributional specifications that are a mix of observed and unobserved elements, as in the case of missing values in response vectors.\n",
    "\n",
    "Model implementation begins by instantiating stochastic and logical nodes using the `Mamba`--supplied constructors ``Stochastic`` and ``Logical``.  Stochastic and logical nodes for a model are defined with a call to the ``Model`` constructor.  The model constructor formally defines and assigns names to the nodes.  User-specified names are given on the left-hand sides of the arguments to which ``Logical`` and ``Stochastic`` nodes are passed.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    using Mamba\n",
    "\n",
    "    ## Model Specification\n",
    "\n",
    "    model = Model(\n",
    "\n",
    "      y = Stochastic(1,\n",
    "        (mu, s2) ->  MvNormal(mu, sqrt(s2)),\n",
    "        false\n",
    "      ),\n",
    "\n",
    "      mu = Logical(1,\n",
    "        (xmat, beta) -> xmat * beta,\n",
    "        false\n",
    "      ),\n",
    "\n",
    "      beta = Stochastic(1,\n",
    "        () -> MvNormal(2, sqrt(1000))\n",
    "      ),\n",
    "\n",
    "      s2 = Stochastic(\n",
    "        () -> InverseGamma(0.001, 0.001)\n",
    "      )\n",
    "\n",
    "    )\n",
    "\n",
    "A single integer value for the first ``Stochastic`` constructor argument indicates that the node is an array of the specified dimension.  Absence of an integer value implies a scalar node.  The next argument is a `function <http://docs.julialang.org/en/latest/manual/functions/>`_ that may contain any valid **julia** code.  Functions should be defined to take, as their arguments, the inputs upon which their nodes depend and, for stochastic nodes, return distribution objects or arrays of objects compatible with the `Distributions` package :cite:`bates:2014:DP`.  Such objects represent the nodes' distributional specifications.  An optional boolean argument after the function can be specified to indicate whether values of the node should be monitored (saved) during MCMC simulations (default: ``true``).\n",
    "\n",
    "Stochastic functions must return a single distribution object that can accommodate the dimensionality of the node, or return an array of (univariate or multivariate) distribution objects of the same dimension as the node.  Examples of alternative, but equivalent, prior distributional specifications for the ``beta`` node are shown below.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    # Case 1: Multivariate Normal with independence covariance matrix\n",
    "    beta = Stochastic(1,\n",
    "      () -> MvNormal(2, sqrt(1000))\n",
    "    )\n",
    "\n",
    "    # Case 2: One common univariate Normal\n",
    "    beta = Stochastic(1,\n",
    "      () -> Normal(0, sqrt(1000))\n",
    "    )\n",
    "\n",
    "    # Case 3: Array of univariate Normals\n",
    "    beta = Stochastic(1,\n",
    "      () -> UnivariateDistribution[Normal(0, sqrt(1000)), Normal(0, sqrt(1000))]\n",
    "    )\n",
    "\n",
    "    # Case 4: Array of univariate Normals\n",
    "    beta = Stochastic(1,\n",
    "      () -> UnivariateDistribution[Normal(0, sqrt(1000)) for i in 1:2]\n",
    "    )\n",
    "\n",
    "Case 1 is one of the `multivariate normal distributions <http://distributionsjl.readthedocs.org/en/latest/multivariate.html#multivariate-normal-distribution>`_ available in the `Distributions` package, and the specification used in the example model implementation.  In Case 2, a single `univariate normal distribution <http://distributionsjl.readthedocs.org/en/latest/univariate.html#normal>`_ is specified to imply independent priors of the same type for all elements of ``beta``.  Cases 3 and 4 explicitly specify a univariate prior for each element of ``beta`` and allow for the possibility of differences among the priors.  Both return `arrays <http://docs.julialang.org/en/latest/manual/arrays/>`_ of Distribution objects, with the last case automating the specification of array elements.\n",
    "\n",
    "In summary, ``y`` and ``beta`` are stochastic vectors, ``s2`` is a stochastic scalar, and ``mu`` is a logical vector.  We note that the model could have been implemented without ``mu``.  It is included here primarily to illustrate use of a logical node.  Finally, note that nodes ``y`` and ``mu`` are not being monitored.\n",
    "\n",
    "\n",
    ".. _section-Line-Schemes:\n",
    "\n",
    "Sampling Schemes\n",
    "^^^^^^^^^^^^^^^^\n",
    "\n",
    "The package provides a flexible system for the specification of schemes to sample stochastic nodes.  Arbitrary blocking of nodes and designation of block-specific samplers is supported.  Furthermore, block-updating of nodes can be performed with samplers provided, defined by the user, or available from other packages.  Schemes are specified as vectors of ``Sampler`` objects.  Constructors are provided for several popular sampling algorithms, including adaptive Metropolis, No-U-Turn (NUTS), and slice sampling.  Example schemes are shown below.  In the first one, NUTS is used for the sampling of ``beta`` and slice for ``s2``.  The two nodes are block together in the second scheme and sampled jointly with NUTS.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Hybrid No-U-Turn and Slice Sampling Scheme\n",
    "    scheme1 = [NUTS(:beta),\n",
    "               Slice(:s2, 3.0)]\n",
    "\n",
    "    ## No-U-Turn Sampling Scheme\n",
    "    scheme2 = [NUTS([:beta, :s2])]\n",
    "\n",
    "Additionally, users are free to create their own samplers with the generic ``Sampler`` constructor.  This is particularly useful in settings were full conditional distributions are of standard forms for some nodes and can be sampled from directly.  Such is the case for the full conditional of :math:`\\bm{\\beta}` which can be written as\n",
    "\n",
    ".. math::\n",
    "\n",
    "    p(\\bm{\\beta} | \\sigma^2, \\bm{y}) &\\propto p(\\bm{y} | \\bm{\\beta}, \\sigma^2) p(\\bm{\\beta}) \\\\\n",
    "    &\\propto \\exp\\left\\{-\\frac{1}{2} (\\bm{\\beta} - \\bm{\\mu})^\\top \\bm{\\Sigma}^{-1} (\\bm{\\beta} - \\bm{\\mu})\\right\\},\n",
    "\n",
    "where :math:`\\bm{\\Sigma} = \\left(\\frac{1}{\\sigma^2} \\bm{X}^\\top \\bm{X} + \\bm{\\Sigma}_\\pi^{-1}\\right)^{-1}` and :math:`\\bm{\\mu} = \\bm{\\Sigma} \\left(\\frac{1}{\\sigma^2} \\bm{X}^\\top \\bm{y} + \\bm{\\Sigma}_\\pi^{-1} \\bm{\\mu}_\\pi\\right)`, and is recognizable as multivariate normal.  Likewise,\n",
    "\n",
    ".. math::\n",
    "\n",
    "    p(\\sigma^2 | \\bm{\\beta}, \\mathbf{y}) &\\propto p(\\bm{y} | \\bm{\\beta}, \\sigma^2) p(\\sigma^2) \\\\\n",
    "    &\\propto \\left(\\sigma^2\\right)^{-(n/2 + \\alpha_\\pi) - 1} \\exp\\left\\{-\\frac{1}{\\sigma^2} \\left(\\frac{1}{2} (\\bm{y} - \\bm{X} \\bm{\\beta})^\\top (\\bm{y} - \\bm{X} \\bm{\\beta}) + \\beta_\\pi \\right) \\right\\},\n",
    "\n",
    "whose form is inverse gamma with :math:`n / 2 + \\alpha_\\pi` shape and :math:`(\\bm{y} - \\bm{X} \\bm{\\beta})^\\top (\\bm{y} - \\bm{X} \\bm{\\beta}) / 2 + \\beta_\\pi` scale parameters.  A user-defined sampling scheme to generate draws from these full conditionals is constructed below.  Another example is given in the :ref:`Pollution <example-Pollution>` example for the sampling of multiple nodes.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## User-Defined Samplers\n",
    "\n",
    "    Gibbs_beta = Sampler([:beta],\n",
    "      (beta, s2, xmat, y) ->\n",
    "        begin\n",
    "          beta_mean = mean(beta.distr)\n",
    "          beta_invcov = invcov(beta.distr)\n",
    "          Sigma = inv(xmat' * xmat / s2 + beta_invcov)\n",
    "          mu = Sigma * (xmat' * y / s2 + beta_invcov * beta_mean)\n",
    "          rand(MvNormal(mu, Sigma))\n",
    "        end\n",
    "    )\n",
    "\n",
    "    Gibbs_s2 = Sampler([:s2],\n",
    "      (mu, s2, y) ->\n",
    "        begin\n",
    "          a = length(y) / 2.0 + shape(s2.distr)\n",
    "          b = sumabs2(y - mu) / 2.0 + scale(s2.distr)\n",
    "          rand(InverseGamma(a, b))\n",
    "        end\n",
    "    )\n",
    "\n",
    "    ## User-Defined Sampling Scheme\n",
    "    scheme3 = [Gibbs_beta, Gibbs_s2]\n",
    "\n",
    "In these samplers, the respective ``MvNormal(2, sqrt(1000))`` and ``InverseGamma(0.001, 0.001)`` priors on stochastic nodes ``beta`` and ``s2`` are accessed directly through the ``distr`` :ref:`fields <section-Stochastic>`.  Features of the `Distributions` objects returned by ``beta.distr`` and ``s2.distr`` can, in turn, be extracted with method functions defined in that package or through their own fields.  For instance, ``mean(beta.distr)`` and ``invcov(beta.distr)`` apply method functions to extract the mean vector and inverse-covariance matrix of the ``beta`` prior.  Whereas, ``shape(s2.distr)`` and ``scale(s2.distr)`` extract the shape and scale parameters from fields of the inverse-gamma prior.  `Distributions` method functions can be found in that package's `documentation <http://distributionsjl.readthedocs.org>`_; whereas, fields are found in the `source code <https://github.com/JuliaStats/Distributions.jl>`_.\n",
    "\n",
    "When possible to do so, direct sampling from full conditions is often preferred in practice because it tends to be more efficient than general-purpose algorithms.  Schemes that mix the two approaches can be used if full conditionals are available for some model parameters but not for others.  Once a sampling scheme is formulated in `Mamba`, it can be assigned to an existing model with a call to the ``setsamplers!`` function.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Sampling Scheme Assignment\n",
    "    setsamplers!(model, scheme1)\n",
    "\n",
    "Alternatively, a predefined scheme can be passed in to the ``Model`` constructor at the time of model implementation as the value to its ``samplers`` argument.\n",
    "\n",
    "\n",
    ".. _section-Line-DAG:\n",
    "\n",
    "Directed Acyclic Graphs\n",
    "-----------------------\n",
    "\n",
    "One of the internal structures created by ``Model`` is a graph representation of the model nodes and their associations.  Graphs are managed internally with the `Graphs` package :cite:`white:2014:GP`.  Like `OpenBUGS`, `JAGS`, and other `BUGS` clones, `Mamba` fits models whose nodes form a directed acyclic graph (DAG).  A *DAG* is a graph in which nodes are connected by directed edges and no node has a path that loops back to itself.  With respect to statistical models, directed edges point from parent nodes to the child nodes that depend on them.  Thus, a child node is independent of all others, given its parents.\n",
    "\n",
    "The DAG representation of a ``Model`` can be printed out at the command-line or saved to an external file in a format that can be displayed with the `Graphviz <http://www.graphviz.org/>`_ software.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Graph Representation of Nodes\n",
    "\n",
    "    >>> draw(model)\n",
    "\n",
    "    digraph MambaModel {\n",
    "      \"mu\" [shape=\"diamond\", fillcolor=\"gray85\", style=\"filled\"];\n",
    "        \"mu\" -> \"y\";\n",
    "      \"xmat\" [shape=\"box\", fillcolor=\"gray85\", style=\"filled\"];\n",
    "        \"xmat\" -> \"mu\";\n",
    "      \"beta\" [shape=\"ellipse\"];\n",
    "        \"beta\" -> \"mu\";\n",
    "      \"s2\" [shape=\"ellipse\"];\n",
    "        \"s2\" -> \"y\";\n",
    "      \"y\" [shape=\"ellipse\", fillcolor=\"gray85\", style=\"filled\"];\n",
    "    }\n",
    "\n",
    "    >>> draw(model, filename=\"lineDAG.dot\")\n",
    "\n",
    "Either the printed or saved output can be passed manually to the Graphviz software to plot a visual representation of the model.  If **julia** is being used with a front-end that supports in-line graphics, like `IJulia` :cite:`johnson:2015:IJ`, and the `GraphViz` **julia** package :cite:`fischer:2014:GV` is installed, then the following code will plot the graph automatically.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    using GraphViz\n",
    "\n",
    "    >>> display(Graph(graph2dot(model)))\n",
    "\n",
    "A generated plot of the regression model graph is show in the figure below.\n",
    "\n",
    ".. figure:: tutorial/lineDAG.png\n",
    "    :align: center\n",
    "\n",
    "    Directed acyclic graph representation of the example regression model nodes.\n",
    "\n",
    "Stochastic, logical, and input nodes are represented by ellipses, diamonds, and rectangles, respectively.  Gray-colored nodes are ones designated as unmonitored in MCMC simulations.  The DAG not only allows the user to visually check that the model specification is the intended one, but is also used internally to check that nodal relationships are acyclic.\n",
    "\n",
    "\n",
    ".. _section-Line-Simulation:\n",
    "\n",
    "MCMC Simulation\n",
    "---------------\n",
    "\n",
    "Data\n",
    "^^^^\n",
    "\n",
    "For the example, observations :math:`(\\bm{x}, \\bm{y})` are stored in a **julia** dictionary defined in the code block below.  Included are predictor and response vectors ``:x`` and ``:y`` as well as a design matrix ``:xmat`` corresponding to the model matrix :math:`\\bm{X}`.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Data\n",
    "    line = Dict{Symbol, Any}(\n",
    "      :x => [1, 2, 3, 4, 5],\n",
    "      :y => [1, 3, 3, 3, 5]\n",
    "    )\n",
    "    line[:xmat] = [ones(5) line[:x]]\n",
    "\n",
    "Initial Values\n",
    "^^^^^^^^^^^^^^\n",
    "\n",
    "A **julia** vector of dictionaries containing initial values for all stochastic nodes must be created.  The dictionary keys should match the node names, and their values should be vectors whose elements are the same type of structures as the nodes.  Three sets of initial values for the regression example are created in with the following code.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Initial Values\n",
    "    inits = [\n",
    "      Dict{Symbol, Any}(\n",
    "        :y => line[:y],\n",
    "        :beta => rand(Normal(0, 1), 2),\n",
    "        :s2 => rand(Gamma(1, 1))\n",
    "      )\n",
    "    for i in 1:3\n",
    "    ]\n",
    "\n",
    "Initial values for ``y`` are those in the observed response vector.  Likewise, the node is not updated in the sampling schemes defined earlier and thus retains its initial values throughout MCMC simulations.  Initial values are generated for ``beta`` from a normal distribution and for ``s2`` from a gamma distribution.\n",
    "\n",
    "MCMC Engine\n",
    "^^^^^^^^^^^\n",
    "\n",
    "MCMC simulation of draws from the posterior distribution of a declared set of model nodes and sampling scheme is performed with the :func:`mcmc` function.  As shown below, the first three arguments are a ``Model`` object, a dictionary of values for input nodes, and a dictionary vector of initial values.  The number of draws to generate in each simulation run is given as the fourth argument.  The remaining arguments are named such that ``burnin`` is the number of initial values to discard to allow for convergence; ``thin`` defines the interval between draws to be retained in the output; and ``chains`` specifies the number of times to run the simulator.  Results are retuned as ``Chains`` objects on which methods for posterior inference are defined.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## MCMC Simulations\n",
    "\n",
    "    setsamplers!(model, scheme1)\n",
    "    sim1 = mcmc(model, line, inits, 10000, burnin=250, thin=2, chains=3)\n",
    "\n",
    "    setsamplers!(model, scheme2)\n",
    "    sim2 = mcmc(model, line, inits, 10000, burnin=250, thin=2, chains=3)\n",
    "\n",
    "    setsamplers!(model, scheme3)\n",
    "    sim3 = mcmc(model, line, inits, 10000, burnin=250, thin=2, chains=3)\n",
    "\n",
    ".. index:: Parallel Computing\n",
    "\n",
    "Parallel Computing\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "The simulation of multiple chains will be executed in parallel automatically if **julia** is running in multiprocessor mode on a multiprocessor system.  Multiprocessor mode can be started with the command line argument ``julia -p n``, where ``n`` is the number of available processors.  See the **julia** documentation on `parallel computing <http://julia.readthedocs.org/en/latest/manual/parallel-computing/>`_ for details.\n",
    "\n",
    "\n",
    ".. _section-Line-Inference:\n",
    "\n",
    "Posterior Inference\n",
    "-------------------\n",
    "\n",
    ".. _section-Line-Diagnostics:\n",
    "\n",
    "Convergence Diagnostics\n",
    "^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Checks of MCMC output should be performed to assess convergence of simulated draws to the posterior distribution.  Checks can be performed with a variety of methods.  The diagnostic of Gelman, Rubin, and Brooks :cite:`gelman:1992:IIS,brooks:1998:GMM` is one method for assessing convergence of posterior mean estimates.  Values of the diagnostic's *potential scale reduction factor (PSRF)* that are close to one suggest convergence.  As a rule-of-thumb, convergence is rejected if the 97.5 percentile of a PSRF is greater than 1.2.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    >>> gelmandiag(sim1, mpsrf=true, transform=true) |> showall\n",
    "\n",
    "    Gelman, Rubin, and Brooks Diagnostic:\n",
    "                  PSRF 97.5%\n",
    "         beta[1] 1.009 1.010\n",
    "         beta[2] 1.009 1.010\n",
    "              s2 1.008 1.016\n",
    "    Multivariate 1.006   NaN\n",
    "\n",
    "The diagnostic of Geweke :cite:`geweke:1992:EAS` tests for non-convergence of posterior mean estimates.  It provides chain-specific test p-values.  Convergence is rejected for significant p-values, like those obtained for ``s2``.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    >>> gewekediag(sim1) |> showall\n",
    "\n",
    "    Geweke Diagnostic:\n",
    "    First Window Fraction = 0.1\n",
    "    Second Window Fraction = 0.5\n",
    "\n",
    "            Z-score p-value\n",
    "    beta[1]   1.237  0.2162\n",
    "    beta[2]  -1.568  0.1168\n",
    "         s2   1.710  0.0872\n",
    "\n",
    "            Z-score p-value\n",
    "    beta[1]  -1.457  0.1452\n",
    "    beta[2]   1.752  0.0797\n",
    "         s2  -1.428  0.1534\n",
    "\n",
    "            Z-score p-value\n",
    "    beta[1]   0.550  0.5824\n",
    "    beta[2]  -0.440  0.6597\n",
    "         s2   0.583  0.5596\n",
    "\n",
    "The diagnostic of Heidelberger and Welch :cite:`heidelberger:1983:SRL` tests for non-convergence (non-stationarity) and whether ratios of estimation interval halfwidths to means are within a target ratio.  Stationarity is rejected (0) for significant test p-values.  Halfwidth tests are rejected (0) if observed ratios are greater than the target, as is the case for ``s2`` and ``beta[1]``.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    >>> heideldiag(sim1) |> showall\n",
    "\n",
    "    Heidelberger and Welch Diagnostic:\n",
    "    Target Halfwidth Ratio = 0.1\n",
    "    Alpha = 0.05\n",
    "\n",
    "            Burn-in Stationarity p-value    Mean     Halfwidth  Test\n",
    "    beta[1]     251            1  0.0680 0.57366275 0.053311283    1\n",
    "    beta[2]     738            1  0.0677 0.81285744 0.015404173    1\n",
    "         s2     738            1  0.0700 1.00825202 0.094300432    1\n",
    "\n",
    "            Burn-in Stationarity p-value    Mean     Halfwidth  Test\n",
    "    beta[1]     251            1  0.1356  0.6293320 0.065092099    0\n",
    "    beta[2]     251            1  0.0711  0.7934633 0.019215278    1\n",
    "         s2     251            1  0.4435  1.4635400 0.588158612    0\n",
    "\n",
    "            Burn-in Stationarity p-value    Mean     Halfwidth  Test\n",
    "    beta[1]     251            1  0.0515  0.5883602 0.058928034    0\n",
    "    beta[2]    1225            1  0.1479  0.8086080 0.018478999    1\n",
    "         s2     251            1  0.6664  0.9942853 0.127959523    0\n",
    "\n",
    "The diagnostic of Raftery and Lewis :cite:`raftery:1992:OLR,raftery:1992:HMI` is used to determine the number of iterations required to estimate a specified quantile within a desired degree of accuracy.  For example, below are required total numbers of iterations, numbers to discard as burn-in sequences, and thinning intervals for estimating 0.025 quantiles so that their estimated cumulative probabilities are within 0.025±0.005 with probability 0.95.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    >>> rafterydiag(sim1) |> showall\n",
    "\n",
    "    Raftery and Lewis Diagnostic:\n",
    "    Quantile (q) = 0.025\n",
    "    Accuracy (r) = 0.005\n",
    "    Probability (s) = 0.95\n",
    "\n",
    "            Thinning Burn-in    Total   Nmin Dependence Factor\n",
    "    beta[1]        2     267      17897 3746         4.7776295\n",
    "    beta[2]        2     267      17897 3746         4.7776295\n",
    "         s2        2     257       8689 3746         2.3195408\n",
    "\n",
    "            Thinning Burn-in    Total   Nmin Dependence Factor\n",
    "    beta[1]        4     271 2.1759x104 3746         5.8085958\n",
    "    beta[2]        4     275 2.8795x104 3746         7.6868660\n",
    "         s2        2     257 8.3450x103 3746         2.2277096\n",
    "\n",
    "            Thinning Burn-in    Total   Nmin Dependence Factor\n",
    "    beta[1]        2     269 2.0647x104 3746         5.5117459\n",
    "    beta[2]        2     263 1.4523x104 3746         3.8769354\n",
    "         s2        2     255 7.8770x103 3746         2.1027763\n",
    "\n",
    "More information on the diagnostic functions can be found in the :ref:`section-Convergence-Diagnostics` section.\n",
    "\n",
    "\n",
    ".. _section-Line-Summaries:\n",
    "\n",
    "Posterior Summaries\n",
    "^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Once convergence has been assessed, sample statistics may be computed on the MCMC output to estimate features of the posterior distribution.  The `StatsBase` package :cite:`lin:2014:SBP` is utilized in the calculation of many posterior estimates.  Some of the available posterior summaries are illustrated in the code block below.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Summary Statistics\n",
    "    >>> describe(sim1)\n",
    "\n",
    "    Iterations = 252:10000\n",
    "    Thinning interval = 2\n",
    "    Chains = 1,2,3\n",
    "    Samples per chain = 4875\n",
    "\n",
    "    Empirical Posterior Estimates:\n",
    "               Mean       SD       Naive SE       MCSE       ESS\n",
    "    beta[1] 0.5971183 1.14894446 0.0095006014 0.016925598 4607.9743\n",
    "    beta[2] 0.8017036 0.34632566 0.0028637608 0.004793345 4875.0000\n",
    "         s2 1.2203777 2.00876760 0.0166104638 0.101798287  389.3843\n",
    "\n",
    "    Quantiles:\n",
    "                2.5%       25.0%       50.0%     75.0%     97.5%\n",
    "    beta[1] -1.74343373 0.026573102 0.59122696 1.1878720 2.8308472\n",
    "    beta[2]  0.12168742 0.628297573 0.80357822 0.9719441 1.5051573\n",
    "         s2  0.17091385 0.383671702 0.65371989 1.2206381 6.0313970\n",
    "\n",
    "    ## Highest Posterior Density Intervals\n",
    "    >>> hpd(sim1)\n",
    "\n",
    "             95% Lower  95% Upper\n",
    "    beta[1] -1.75436235 2.8109571\n",
    "    beta[2]  0.09721501 1.4733163\n",
    "         s2  0.08338409 3.8706865\n",
    "\n",
    "    ## Cross-Correlations\n",
    "    >>> cor(sim1)\n",
    "\n",
    "               beta[1]      beta[2]        s2\n",
    "    beta[1]  1.000000000 -0.905245029  0.027467317\n",
    "    beta[2] -0.905245029  1.000000000 -0.024489462\n",
    "         s2  0.027467317 -0.024489462  1.000000000\n",
    "\n",
    "    ## Lag-Autocorrelations\n",
    "    >>> autocor(sim1)\n",
    "\n",
    "               Lag 2       Lag 10        Lag 20       Lag 100\n",
    "    beta[1] 0.24521566  -0.021411797 -0.0077424153  -0.044989417\n",
    "    beta[2] 0.20402485  -0.019107846  0.0033980453  -0.053869216\n",
    "         s2 0.85931351   0.568056917  0.3248136852   0.024157524\n",
    "\n",
    "               Lag 2       Lag 10        Lag 20       Lag 100\n",
    "    beta[1] 0.28180489  -0.031007672    0.03930888  0.0394895028\n",
    "    beta[2] 0.25905976  -0.017946010    0.03613043  0.0227758214\n",
    "         s2 0.92905843   0.761339226    0.58455868  0.0050215824\n",
    "\n",
    "               Lag 2       Lag 10        Lag 20       Lag 100\n",
    "    beta[1] 0.38634357 -0.0029361782  -0.032310111  0.0028806786\n",
    "    beta[2] 0.32822879 -0.0056670786  -0.020100663 -0.0062622517\n",
    "         s2 0.68812720  0.2420402859   0.080495078 -0.0290205896\n",
    "\n",
    "    ## State Space Change Rate (per Iteration)\n",
    "    >>> changerate(sim1)\n",
    "\n",
    "                 Change Rate\n",
    "         beta[1]       0.844\n",
    "         beta[2]       0.844\n",
    "              s2       1.000\n",
    "    Multivariate       1.000\n",
    "\n",
    "    ## Deviance Information Criterion\n",
    "    >>> dic(sim1)\n",
    "\n",
    "          DIC    Effective Parameters\n",
    "    pD 13.828540            1.1661193\n",
    "    pV 22.624104            5.5639015\n",
    "\n",
    "\n",
    ".. _section-Line-Subsetting:\n",
    "\n",
    "Output Subsetting\n",
    "^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Additionally, sampler output can be subsetted to perform posterior inference on select iterations, parameters, and chains.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Subset Sampler Output\n",
    "    >>> sim = sim1[1000:5000, [\"beta[1]\", \"beta[2]\"], :]\n",
    "    >>> describe(sim)\n",
    "\n",
    "    Iterations = 1000:5000\n",
    "    Thinning interval = 2\n",
    "    Chains = 1,2,3\n",
    "    Samples per chain = 2001\n",
    "\n",
    "    Empirical Posterior Estimates:\n",
    "               Mean        SD      Naive SE      MCSE       ESS\n",
    "    beta[1] 0.62445845 1.0285709 0.013275474 0.023818436 1864.8416\n",
    "    beta[2] 0.79392648 0.3096614 0.003996712 0.006516677 2001.0000\n",
    "\n",
    "    Quantiles:\n",
    "                2.5%       25.0%       50.0%     75.0%     97.5%\n",
    "    beta[1] -1.53050898 0.076745702 0.61120944 1.2174641 2.6906753\n",
    "    beta[2]  0.18846617 0.618849048 0.79323126 0.9619767 1.4502109\n",
    "\n",
    "\n",
    ".. _section-Line-FileIO:\n",
    "\n",
    "File I/O\n",
    "^^^^^^^^\n",
    "\n",
    "For cases in which it is desirable to store sampler output in external files for processing in future **julia** sessions, read and write methods are provided.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Write to and Read from an External File\n",
    "    write(\"sim1.jls\", sim1)\n",
    "    sim1 = read(\"sim1.jls\", ModelChains)\n",
    "\n",
    "\n",
    "Restarting the Sampler\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Convergence diagnostics or posterior summaries may indicate that additional draws from the posterior are needed for inference.  In such cases, the :func:`mcmc` function can be used to restart the sampler with previously generated output, as illustrated below.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Restart the Sampler\n",
    "    >>> sim = mcmc(sim1, 5000)\n",
    "    >>> describe(sim)\n",
    "\n",
    "    Iterations = 252:15000\n",
    "    Thinning interval = 2\n",
    "    Chains = 1,2,3\n",
    "    Samples per chain = 7375\n",
    "\n",
    "    Empirical Posterior Estimates:\n",
    "               Mean        SD      Naive SE       MCSE        ESS\n",
    "    beta[1] 0.59655228 1.1225920 0.0075471034 0.014053505 6380.79199\n",
    "    beta[2] 0.80144540 0.3395731 0.0022829250 0.003954871 7372.28048\n",
    "         s2 1.18366563 1.8163096 0.0122109158 0.070481708  664.08995\n",
    "\n",
    "    Quantiles:\n",
    "                2.5%       25.0%       50.0%     75.0%     97.5%\n",
    "    beta[1] -1.70512374 0.031582137 0.58989089 1.1783924 2.8253668\n",
    "    beta[2]  0.12399073 0.630638800 0.80358526 0.9703569 1.4939817\n",
    "         s2  0.17075261 0.382963160 0.65372440 1.2210168 5.7449800\n",
    "\n",
    "\n",
    ".. _section-Line-Plotting:\n",
    "\n",
    "Plotting\n",
    "^^^^^^^^\n",
    "\n",
    "Plotting of sampler output in `Mamba` is based on the `Gadfly` package :cite:`jones:2014:GP`.  Summary plots can be created and written to files using the ``plot`` and ``draw`` functions.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Default summary plot (trace and density plots)\n",
    "    p = plot(sim1)\n",
    "\n",
    "    ## Write plot to file\n",
    "    draw(p, filename=\"summaryplot.svg\")\n",
    "\n",
    ".. figure:: tutorial/summaryplot.*\n",
    "    :align: center\n",
    "\n",
    "    Trace and density plots.\n",
    "\n",
    "The ``plot`` function can also be used to make autocorrelation and running means plots.  Legends can be added with the optional ``legend`` argument.  Arrays of plots can be created and passed to the ``draw`` function.  Use ``nrow`` and  ``ncol`` to determine how many rows and columns of plots to include in each drawing.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Autocorrelation and running mean plots\n",
    "    p = plot(sim1, [:autocor, :mean], legend=true)\n",
    "    draw(p, nrow=3, ncol=2, filename=\"autocormeanplot.svg\")\n",
    "\n",
    ".. figure:: tutorial/autocormeanplot.*\n",
    "    :align: center\n",
    "\n",
    "    Autocorrelation and running mean plots.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Pairwise contour plots\n",
    "    p = plot(sim1, :contour)\n",
    "    draw(p, nrow=2, ncol=2, filename=\"contourplot.svg\")\n",
    "\n",
    ".. figure:: tutorial/contourplot.*\n",
    "    :align: center\n",
    "\n",
    "    Pairwise posterior density contour plots.\n",
    "\n",
    "\n",
    ".. _section-Line-Performance:\n",
    "\n",
    "Computational Performance\n",
    "-------------------------\n",
    "\n",
    "Computing runtimes were recorded for different sampling algorithms applied to the regression example.  Runs wer performed on a desktop computer with an Intel i5-2500 CPU @ 3.30GHz.  Results are summarized in the table below.  Note that these are only intended to measure the raw computing performance of the package, and do not account for different efficiencies in output generated by the sampling algorithms.\n",
    "\n",
    ".. table:: Number of draws per second for select sampling algorithms in `Mamba`.\n",
    "\n",
    "    +--------------+--------------+--------+-------+--------------+--------------+\n",
    "    | Adaptive Metropolis         |        |       | Slice                       |\n",
    "    +--------------+--------------+--------+-------+--------------+--------------+\n",
    "    | Within Gibbs | Multivariate | Gibbs  | NUTS  | Within Gibbs | Multivariate |\n",
    "    +==============+==============+========+=======+==============+==============+\n",
    "    | 16,700       | 11,100       | 27,300 | 2,600 | 13,600       | 17,600       |\n",
    "    +--------------+--------------+--------+-------+--------------+--------------+\n",
    "\n",
    "\n",
    ".. _section-Line-Development:\n",
    "\n",
    "Development and Testing\n",
    "-----------------------\n",
    "\n",
    "Command-line access is provided for all package functionality to aid in the development and testing of models.  Examples of available functions are shown in the code block below.  Documentation for these and other related functions can be found in the :ref:`section-MCMC-Types` section.\n",
    "\n",
    ".. code-block:: julia\n",
    "\n",
    "    ## Development and Testing\n",
    "\n",
    "    setinputs!(model, line)             # Set input node values\n",
    "    setinits!(model, inits[1])          # Set initial values\n",
    "    setsamplers!(model, scheme1)        # Set sampling scheme\n",
    "\n",
    "    showall(model)                      # Show detailed node information\n",
    "\n",
    "    logpdf(model, 1)                    # Log-density sum for block 1\n",
    "    logpdf(model, 2)                    # Block 2\n",
    "    logpdf(model)                       # All blocks\n",
    "\n",
    "    sample!(model, 1)                  # Sample values for block 1\n",
    "    sample!(model, 2)                  # Block 2\n",
    "    sample!(model)                     # All blocks\n",
    "\n",
    "In this example, functions ``setinputs!``, ``setinits!``, and ``setsampler!`` allow the user to manually set the input node values, the initial values, and the sampling scheme form the ``model`` object, and would need to be called prior to ``logpdf`` and ``sample!``.  Updated model objects should be returned when called; otherwise, a problem with the supplied values may exist.  Method ``showall`` prints a detailed summary of all model nodes, their values, and attributes; ``logpdf`` sums the log-densities over nodes associated with a specified sampling block (second argument); and ``sample!`` generates an MCMC sample of values for the nodes.  Non-numeric results may indicate problems with distributional specifications in the second case or with sampling functions in the last case.  The block arguments are optional; and, if left unspecified, will cause the corresponding functions to be applied over all sampling blocks.  This allows testing of some or all of the samplers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.5",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
